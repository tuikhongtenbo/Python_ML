{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Component Analysis (PCA) via Maximum Variance Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an observed dataset $\\mathcal{D} = \\{x_1, x_2, ..., x_N\\}$ where $x_i \\in \\mathbb{R}^D$ in a Euclidean space. Defining the avarage of the dataset as:\n",
    "\n",
    "$$\n",
    "    \\bar{x} = \\frac{1}{N} \\sum_{n=1}^{N} x_n\n",
    "$$\n",
    "and the covariance as:\n",
    "\n",
    "$$\n",
    "    S = \\frac{1}{N} \\sum_{n=1}^N (x_n - \\bar{x})(x_n - \\bar{x})^T\n",
    "$$\n",
    "\n",
    "Our goal is to project the dataset having dimensional space $D \\in \\mathbb{N}$ onto a subspace with lower dimension $M << D$ but keep the most variation of the dataset. This means we want to determine a linear transformation that map every data point in $\\mathcal{D}$ into a subspace that keep the most features of the original space.\n",
    "\n",
    "For simplicity, let consider the case where $M = 1$. Let define the linear transformation via an unit vector (for later convenience) $u \\in \\mathbb{R}^D$ so that we have $u^T u = \\parallel u \\parallel$. Via this linear transformation, the projected data point $u_i = u^T x_i$ has the mean of:\n",
    "\n",
    "$$\n",
    "    \\bar{u} = \\frac{1}{N} \\sum_{n=1}^N u_n = \\frac{1}{N} \\sum_{n=1}^N u^T x_n = u^T \\frac{1}{N} \\sum_{n=1}^N x_n = u^T \\bar{x}\n",
    "$$\n",
    "and the respective variance is:\n",
    "\n",
    "$$\n",
    "    \\mathcal{S} = \\frac{1}{N} \\sum_{n=1}^N \\parallel u_n - \\bar{u} \\parallel^2 = \\frac{1}{N} \\sum_{n=1}^N \\parallel u^T x_n - u^T \\bar{x} \\parallel^2 = u^T S u\n",
    "$$\n",
    "\n",
    "According to our target, we need to find the linear transformation $u$ such that it maximize the variance $\\mathcal{S}$. However, we have to constraint $u$ to avoid the case that $u \\rightarrow +\\infty$ is obviously maximize the variance $\\mathcal{S}$. To this end, we introduce the objective function under the Larange Multiplier (see below):\n",
    "\n",
    "$$\n",
    "    L(S, u, \\lambda) = u^T S u + \\lambda (1 - u^T u)\n",
    "$$\n",
    "where $\\lambda \\in \\mathbb{R}$.\n",
    "\n",
    "As both two terms of $L(S, u, \\lambda)$ has the quadratic form, there is only one optimal solution for this function. Taking the derivation with respect to (w.r.t) $u$ we have:\n",
    "\n",
    "$$\n",
    "    \\frac{\\delta}{\\delta u} L(S, u, \\lambda) = 2Su - 2\\lambda u\n",
    "$$\n",
    "\n",
    "Setting this partial derivative to $0$ we have:\n",
    "\n",
    "\\begin{align}\n",
    "    &                   & \\frac{\\delta}{\\delta u} L(S, u, \\lambda)    & = 0 \\\\\n",
    "    & \\Leftrightarrow   & 2Su - 2\\lambda u                            & = 0 \\\\\n",
    "    & \\Leftrightarrow   & Su                                          & = \\lambda u\n",
    "\\end{align}\n",
    "\n",
    "That means the linear transformation $u$ is a eigenvector of the variance matrix $S$. If the left-multiple $(3)$ with $u$ and take advantage of $u^T u = 1$ then we have\n",
    "\n",
    "$$\n",
    "    u^T S u = \\mathcal{S} = \\lambda\n",
    "$$\n",
    "\n",
    "Hence to maximize the variance of projected data $\\mathcal{S}$ we need to find the maximum eigenvalue of $S$. The eigentvector w.r.t to this eigenvalue is known as the first principle component. From that on, we can define additional components in an increasing fashion so that the new principle components are orthogonal to the selected principle components and maximize the variance in those direction. This way can be proved easily via induction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Component Analysis (PCA) via Minimum-Error Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view PCA as an attempt to minizing the projection error. First we introduce the completed orthogonal base of vector space $\\mathbb{R}^D$ as $\\mathcal{U} = \\{u_i\\}_{1 \\le i \\le D}$ which satisfies $u_i^Tu_j = \\delta(i, j)$ (the Kronecker function). From that on, we have:\n",
    "\n",
    "$$\n",
    "    x_n = \\sum_{i=1}^D \\alpha_{ni} u_i\n",
    "$$\n",
    "for any $x_n \\in \\mathcal{D}$.\n",
    "\n",
    "As the base $\\mathcal{U}$ is completed and orthogonal, we have\n",
    "\n",
    "$$\n",
    "    \\alpha_{ni} = u_i^T x_n\n",
    "$$\n",
    "so that\n",
    "\n",
    "$$\n",
    "    x_n = \\sum_{i=1}^D (u_i^T x_n) u_i\n",
    "$$\n",
    "\n",
    "We want to represent $x_n$ in a subspace of dimention $M << D$, leaving the remaining dimension as constant across data points. Therefore we can define the novel form of representation as\n",
    "\n",
    "$$\n",
    "    \\tilde{x}_n = \\sum_{i=1}^M z_{ni} u_i + \\sum_{i=M+1}^D b_i u_i\n",
    "$$\n",
    "where $z_{ni} \\in \\mathbb{R}$ are depent on each $\\tilde{x}_n$ and $b_i \\in \\mathbb{R}$ are constants.\n",
    "\n",
    "The objective function in our case is defined as:\n",
    "$$\n",
    "    J = \\frac{1}{N} \\sum_{n=1}^N \\parallel x_n - \\tilde{x}_n \\parallel^2\n",
    "$$\n",
    "This formula has the quadratic form so there only one possible minimal solution.\n",
    "\n",
    "Taking the partial derivative of $J$ w.r.t $z_{ni}$ we have:\n",
    "\n",
    "\\begin{align}\n",
    "    & \\frac{\\delta J}{\\delta z_{ni}} & = & \\frac{1}{N} \\sum_{m=1}^N \\frac{\\delta}{\\delta z_{ni}} \\parallel x_m - \\tilde{x}_m \\parallel^2 \\\\\n",
    "    & & = & \\frac{1}{N} \\frac{\\delta}{\\delta z_{ni}} \\parallel x_n - \\tilde{x}_n \\parallel^2\n",
    "\\end{align}\n",
    "\n",
    "Let examine $\\frac{\\delta}{\\delta z_{ni}} \\parallel x_n - \\tilde{x}_n \\parallel^2$ first, we have:\n",
    "\n",
    "\\begin{align}\n",
    "    &   \\frac{\\delta}{\\delta z_{ni}} \\parallel x_n - \\tilde{x}_n \\parallel^2 \\\\\n",
    "    & = 2 (x_n - \\tilde{x}_n) \\frac{\\delta}{\\delta z_{ni}} (x_n - \\tilde{x}_n) \\\\\n",
    "    & = 2 (x_n - \\tilde{x}_n) \\frac{\\delta}{\\delta z_{ni}} \\left( \\sum_{k=1}^D (u_k^T x_n)u_k - \\sum_{k=1}^M z_{nk} u_k - \\sum_{k=M+1}^D b_k u_k \\right) \\\\\n",
    "    & = 2 (x_n - \\tilde{x}_n) (- u_i) \\\\\n",
    "    & = 2 (\\tilde{x}_n - x_n)u_i\n",
    "\\end{align}\n",
    "Applying the orthogonal property of the base $\\mathcal{U}$ we have:\n",
    "\n",
    "$$\n",
    "    \\frac{\\delta}{\\delta z_{ni}} \\parallel x_n - \\tilde{x}_n \\parallel^2 = 2 (z_{ni} - u_i^T x_n)\n",
    "$$\n",
    "To this end, we have:\n",
    "\n",
    "$$\n",
    "    \\frac{\\delta J}{\\delta z_{ni}} = \\frac{2}{N} (z_{ni} - u_i^T x_n)\n",
    "$$\n",
    "\n",
    "Taking this partial derivative to $0$ we have:\n",
    "\n",
    "$$\n",
    "    \\frac{\\delta J}{\\delta z_{ni}} = \\frac{2}{N} (z_{ni} - u_i^T x_n) \\Leftrightarrow (z_{ni} - u_i^T x_n) = 0 \\Leftrightarrow z_{ni} = u_i^T x_n\n",
    "$$\n",
    "\n",
    "Apply the same method with the note that $b_i$ independend to $x_n$, we have:\n",
    "\n",
    "\\begin{align}\n",
    "    & \\frac{\\delta J}{\\delta b_i} & = & \\frac{1}{N} \\sum_{n=1}^N \\frac{\\delta}{\\delta b_i} \\parallel x_n - \\tilde{x}_n \\parallel^2 \\\\\n",
    "    & & = & \\frac{1}{N} \\sum_{n=1}^N \\frac{\\delta}{\\delta b_i} \\parallel x_n - \\tilde{x}_n \\parallel^2 \\\\\n",
    "    & & = & \\frac{1}{N} \\sum_{n=1}^N 2 (x_n - \\tilde{x}_n) \\frac{\\delta}{\\delta b_i} (x_n - \\tilde{x}_n) \\\\\n",
    "    & & = & \\frac{1}{N} \\sum_{n=1}^N 2 (x_n - \\tilde{x}_n) \\frac{\\delta}{\\delta b_i} \\left( \\sum_{k=1}^D (u_k^T x_n)u_k - \\sum_{k=1}^M z_{nk} u_k - \\sum_{k=M+1}^D b_k u_k \\right) \\\\\n",
    "    & & = & \\frac{1}{N} \\sum_{n=1}^N 2 (x_n - \\tilde{x}_n) \\frac{\\delta}{\\delta b_i} \\sum_{k=M+1}^D (u_k^T x_k - b_k) u_k \\\\\n",
    "    & & = & \\frac{1}{N} \\sum_{n=1}^N 2 (x_n - \\tilde{x}_n)  \\sum_{k=M+1}^D \\frac{\\delta}{\\delta b_i} (u_k^T x_k - b_k) u_k \\\\\n",
    "    & & = & \\frac{1}{N} \\sum_{n=1}^N 2 (x_n - \\tilde{x}_n)  \\sum_{k=M+1}^D (-u_k) \\\\\n",
    "    & & = & \\frac{1}{N} \\sum_{n=1}^N 2 (u_n^T x_n - b_i) \\\\\n",
    "    & & = & \\frac{1}{N} \\sum_{n=1}^N 2 u_n^T x_n - b_i \\\\\n",
    "    & & = & u_n^T \\bar{x} - b_i \\\\\n",
    "\\end{align}\n",
    "\n",
    "Setting this partial derivative to $0$ we have:\n",
    "\n",
    "$$\n",
    "    \\frac{\\delta J}{\\delta b_i} = 0 \\Leftrightarrow u_n^T \\bar{x} - b_i = 0 \\Leftrightarrow b_i = u_n^T \\bar{x}\n",
    "$$\n",
    "\n",
    "Substituting $z_{ni}$ and $b_i$ to $x_n - \\tilde{x}_n$ we have:\n",
    "$$\n",
    "    x_n - \\tilde{x}_n = \\sum_{n=M+1}^D \\left((x_n - \\bar{x}_n)^T u_i\\right) u_i\n",
    "$$\n",
    "We can see that the displacement between $x_n$ and the $\\tilde{x}_n$ lies in the orthogonal space to the principle subspace. Therefore vectors in the principle subspace can move freely without any effect to the minimum errors.\n",
    "\n",
    "Having the form of $z_{ni}$ and $b_i$, we rewrite the objective function as following\n",
    "\n",
    "$$\n",
    "    J = \\frac{1}{N} \\sum_{n=1}^N \\sum_{i=M+1}^D (u_i^T x_n - u_i^T \\bar{x})^2 = \\sum_{i=M+1}^D u_i^T S u_i = u^T S u\n",
    "$$\n",
    "where $u = (u_{M+1}, u_{M+2}, ..., u_D)^T \\in \\mathbb{R}^{(D-M) \\times D}$.\n",
    "\n",
    "Again, we need to avoid the case $u = 0$ by introducing the Largange Multiplier as follows:\n",
    "\n",
    "$$\n",
    "    L(S, u, \\lambda_1, \\lambda_2, ..., \\lambda_{D-M}) = u^T S u + \\sum_{i=1}^{D-M} \\lambda_i (1 - u_{M+i}^T u_{M+i})\n",
    "$$\n",
    "For each $i \\in \\{M+1, M+2, ..., D\\}$, let take the partial derivative w.r.t $u_i$ and set this to $0$, we have:\n",
    "\\begin{align}\n",
    "    &                   & \\frac{\\delta}{\\delta u_i} L(S, u, \\lambda_1, \\lambda_2, ..., \\lambda_{D-M}) & = & 0 \\\\\n",
    "    & \\Leftrightarrow   & 2S u_{M+i} - 2\\lambda_i u_{M+i} & = & 0 \\\\\n",
    "    & \\Leftrightarrow   & S u_{M+i} & = & \\lambda_i u_{M+i}\n",
    "\\end{align}\n",
    "In the other hand, $u_{M+i}$ is eigenvector of covariance matrix $S$ with $\\lambda_i$ is the respective eigenvalue.\n",
    "\n",
    "Finally, the corresponding value of the objective function $J$ is given as:\n",
    "$$\n",
    "    J = \\sum_{i=M+1}^D u_i^T S u_i = \\sum_{i=M+1}^D u_i^T \\lambda_{M-i} u_i = \\sum_{i=M+1}^D \\lambda_{M-i}\n",
    "$$\n",
    "that is, $J$ reaches its minimum value when the principle subspace is generated from the eigenvectors of $S$ having respective maximum eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Principle Component Analysis (PPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interprete the PCA process as finding a latent variable having lower dimension that keep the most information of the given input. Formally, we can see PCA from the generative viewpoint that a sampled value of the observed variable $x \\in \\mathbb{R}^D$ is obtained by first choosing a value for a sampled **latent variable** $z \\in \\mathbb{R}^M$ where $M << D$:\n",
    "\n",
    "$$\n",
    "    x = Wz + \\mu + \\varepsilon\n",
    "$$\n",
    "where $W \\in \\mathbb{R}^{D \\times M}$, $\\mu \\in \\mathbb{R}^D$ and\n",
    "$$\n",
    "    z \\sim \\mathcal{N}(0, I_D)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_D)\n",
    "$$\n",
    "\n",
    "We can easily see that\n",
    "\n",
    "\\begin{equation}\n",
    "    P(x|z) = \\mathcal{N}(x | Wz + \\mu, \\sigma^2 I_D)\n",
    "\\end{equation}\n",
    "\n",
    "The model (1) with parameters $W, \\mu, \\sigma$ is a form of linear-Gausssian model. PCA constructed in this view is called the probalistic PCA with latent variable approach. In this probabilistic form, we can approximate the parameters $W, \\sigma$, and $\\mu$ using the maximum likelihood.\n",
    "\n",
    "Before going to the maximum likelihood estimation, we determine the marginal probability $P(x)$. This can be obtained easily via the sum and product rule in probabilistic theory:\n",
    "\n",
    "$$\n",
    "    P(x) = \\int P(x|z)P(z)dz\n",
    "$$\n",
    "As we have defined both $P(x|z)$ and $P(z)$ in the form of Gaussian model, this likelihood will have the form of Gaussian model:\n",
    "\n",
    "$$\n",
    "    P(x) = \\mathcal{N}(x|\\mu, C)\n",
    "$$\n",
    "where $C = WW^T + \\sigma^2I_D \\in \\mathbb{R}^{D \\times D}$ is the covariance matrix. To see how we can obtain this form of expectation and covariance, let examine:\n",
    "\n",
    "$$\n",
    "    \\mathbb{E}[x] = \\mathbb{E}[Wz + \\mu + \\varepsilon] = \\mu\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "    Cov[x]  & = \\mathbb{E}[(x - \\mathbb{E}[x])(x - \\mathbb{E}[x])^T] \\\\\n",
    "            & = \\mathbb{E}[(Wz + \\varepsilon)(Wz + \\varepsilon)^T] \\\\\n",
    "            & = \\mathbb{E}[Wz z^T W^T] + \\mathbb{E}[\\varepsilon\\varepsilon^T] \\\\\n",
    "            & = WW^T + \\sigma^2 I_D\n",
    "\\end{align}\n",
    "\n",
    "Giving the parameters $W, \\mu$, and $\\sigma$ of the Gaussian probability $P(C|W, \\mu, \\sigma)$, we can apply the maximum likelihood estimator to have the log likehood function as:\n",
    "\n",
    "$$\n",
    "    ln \\left[P(X|W, \\mu, \\sigma)\\right] = \\sum_{n=1}^N ln \\left[P(x_n|W, \\mu, \\sigma)\\right] = \\sum_{n=1}^N ln\\left[\\mathcal{N}(x_n|W, \\mu, \\sigma)\\right]\n",
    "$$\n",
    "\n",
    "As each $ln \\left[\\mathcal{N}(x_n|W, \\mu, \\sigma)\\right]$ has the form\n",
    "\n",
    "$$\n",
    "    ln \\left[\\mathcal{N}(x_n|W, \\mu, \\sigma)\\right] = -\\frac{D}{2} ln|2\\pi| - \\frac{1}{2}ln|C| - \\frac{1}{2} (x_n - \\mu)^T C^{-1} (x_n - \\mu)\n",
    "$$\n",
    "we have:\n",
    "$$\n",
    "    ln \\left[P(X|W, \\mu, \\sigma)\\right] = \\frac{DN}{2} ln |2\\pi| - \\frac{N}{2}ln|C| - \\frac{1}{2}\\sum_{n=1}^N (x_n - \\mu)^T C^{-1} (x_n - \\mu)\n",
    "$$\n",
    "\n",
    "The maximum likelihood (ML) solution of the $\\mu$ can be found by setting the derivative of the log likelihood w.r.t $\\mu$ to $0$, that is:\n",
    "\n",
    "\\begin{align}\n",
    "    &                   & \\frac{\\delta}{\\delta \\mu} ln \\left[P(X|W, \\mu, \\sigma^2)\\right] & = 0 \\\\\n",
    "    & \\Leftrightarrow   & \\mu C^{-1}\\sum_{n=1}^N (x_n - \\mu) & = 0 \\\\\n",
    "    & \\Leftrightarrow   & \\sum_{n=1}^N (x_n - \\mu) & = 0 \\\\\n",
    "    & \\Leftrightarrow   & \\mu & = \\frac{1}{N} \\sum_{n=1}^N x_n  \\\\\n",
    "    & \\Rightarrow   & \\mu_{ML} & = \\bar{x}  \\\\\n",
    "\\end{align}\n",
    "\n",
    "Replacing $\\mu_{ML}$ back to the log likelihood function, we have\n",
    "\n",
    "\\begin{align}\n",
    "    ln \\left[P(X|W, \\mu, \\sigma)\\right] & = -\\frac{DN}{2} ln|2\\pi| - \\frac{N}{2}ln|C| - \\frac{1}{2}\\sum_{n=1}^N (x_n - \\bar{x})^T C^{-1} (x_n - \\bar{x}) \\\\\n",
    "                                        & = -\\frac{DN}{2} \\ln|2\\pi| - \\frac{N}{2}ln|C| - \\frac{N}{2}Tr(C^{-1} S) \\\\\n",
    "                                        & = - \\frac{N}{2}\\left[(Dln|2\\pi| + ln|C| + Tr(C^{-1} S))\\right] \\\\\n",
    "\\end{align}\n",
    "where $Tr((a_{ij})_{ij}) = \\sum_{n=1}^D a_{ii}$ with $(a_{ij})_{ij} \\in \\mathbb{R}^{D \\times D}$. We have the trace form of matrix because of the assumption where each $x_i, x_j$ are independent to each other, hence $Var[x_i, x_j] = 0$ for any $j \\ne j$. Accordingly, the log likelihood now has the form of quadratic function, hence it has an unique optimal solution of $\\mu$.\n",
    "\n",
    "Similarly, to obtain the ML estimation of parameter $W$, we have to set the partial derivative of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Lagrange Multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
